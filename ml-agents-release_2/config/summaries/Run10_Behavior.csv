Steps,Policy/Entropy,Policy/Extrinsic Value Estimate,Environment/Episode Length,Environment/Cumulative Reward,Policy/Extrinsic Reward,Losses/Value Loss,Losses/Policy Loss,Policy/Learning Rate,Is Training
5000,4.9650373,0.16381381,147.5151515151515,0.07575757598335092,0.07575757598335092,0.0065773614,0.06878157,0.0002999814,1.0
10000,4.951561,0.09586858,121.7560975609756,0.03414634142707034,0.03414634142707034,0.0035607219,0.068891466,0.00029995636,1.0
15000,4.94324,0.027499946,119.64285714285714,-0.01463414637780771,-0.01463414637780771,0.0015636971,0.06877681,0.00029992504,1.0
20000,4.948058,0.005530174,108.8409090909091,-0.05333333412806193,-0.05333333412806193,0.00042159914,0.07014683,0.00029989402,1.0
25000,4.9492755,0.0050613214,164.58064516129033,0.05806451579255442,0.05806451579255442,0.0013959053,0.069304526,0.00029986317,1.0
30000,4.943552,0.010489879,154.75,0.09687500074505806,0.09687500074505806,0.001889809,0.08136259,0.000299832,1.0
35000,4.933366,0.0039149676,132.77777777777777,0.03611111123528746,0.03611111123528746,0.0018658711,0.065313056,0.00029980682,1.0
40000,4.918307,0.0975459,421.25,0.691666675110658,0.691666675110658,0.00372439,0.06822038,0.0002997756,1.0
45000,4.931693,0.005365526,116.43181818181819,-0.011627907323282819,-0.011627907323282819,0.0014853324,0.0704279,0.00029974457,1.0
