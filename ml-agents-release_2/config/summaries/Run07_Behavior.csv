Steps,Policy/Entropy,Policy/Extrinsic Value Estimate,Environment/Episode Length,Environment/Cumulative Reward,Policy/Extrinsic Reward,Losses/Value Loss,Losses/Policy Loss,Policy/Learning Rate,Is Training
10000,4.9538193,-0.04687867,87.73451327433628,-1.0178571428571428,-1.0178571428571428,0.118556514,0.073263384,0.0002999689,1.0
20000,4.8930554,-0.43549618,107.97727272727273,-1.101123595505618,-1.101123595505618,0.05080261,0.065607175,0.00029991305,1.0
30000,4.8235755,-0.56694996,139.32432432432432,-1.0410958904109588,-1.0410958904109588,0.07870633,0.070420206,0.00029985042,1.0
40000,4.8022304,-0.57794005,135.76470588235293,-0.9855072463768116,-0.9855072463768116,0.02859814,0.06504288,0.0002997877,1.0
50000,4.743782,-0.6297295,131.9125,-1.025,-1.025,0.030201111,0.066040814,0.00029972522,1.0
60000,4.5152054,-0.46867284,177.57142857142858,-1.0357142857142858,-1.0357142857142858,0.025092073,0.07376705,0.00029966925,1.0
